{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate to Univariate Prediction\n",
    "**This notebook compares the current LSTM model trained on emotion data against two naive models**:\n",
    "\n",
    "1. Mean Model: Takes the mean of all timesteps in the lookback period.\n",
    "2. Learnable Weighted Mean Model: Computes a weighted mean of all previous timesteps, where the weights are learned.\n",
    "\n",
    "\n",
    "\n",
    "### For prediction, I will explore two approaches:\n",
    "1. Recursive Single Prediction: Predicts future time steps one at a time using previous prediction as input for the next prediction.\n",
    "2. All-Timestep Prediction: Predicts all future timesteps in one shot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/Users/arnav/Google Drive/Cornell/Coding Projects/emili_TimeSeriesPredictor') #going up several files until emili_TimeSeriesPredictor\n",
    "from time_series_predictor.Data.emotionFeatureExtractor import emotionFeatureExtractor\n",
    "#Modeling imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Input, Lambda, Flatten\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "import tensorflow.keras.backend as K\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generating Synthetic Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_t = 0.1\n",
    "synthetic_frequency = 10\n",
    "lookback = 600\n",
    "forecast = 1\n",
    "stride = lookback + forecast #so no overlap\n",
    "\n",
    "nSamples = 30\n",
    "noise_lvl = 0.0\n",
    "\n",
    "print(f'Number of seconds of data is {delta_t*(lookback+forecast)*nSamples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Univariate synthetic data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_univariate_synthetic_data(timesteps, delta_t, frequency=0.001, noise_level=0.1):\n",
    "    t = np.linspace(0, (timesteps-1)*delta_t, timesteps)\n",
    "\n",
    "    # Generate sine wave based on frequency, where `frequency` is in Hz\n",
    "    signal = np.sin(2 * np.pi * frequency * t)\n",
    "    \n",
    "    # Add noise to the signal\n",
    "    noise = np.random.normal(0, noise_level, timesteps)\n",
    "\n",
    "    return t, signal + noise\n",
    "\n",
    "t,univariate_data = gen_univariate_synthetic_data(timesteps = (lookback+forecast)*nSamples, delta_t=delta_t, noise_level=noise_lvl)\n",
    "\n",
    "plt.plot(t, univariate_data)\n",
    "plt.title(f\"Univariate Time Series: Sum of Sine and Cosine + Noise (delta_t={delta_t})\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Signal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating train-test-split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split(data, lookback_period, forecast_period, stride, test_split = 0.2):\n",
    "    # Ensure data is a numpy array\n",
    "    data = np.array(data)\n",
    "\n",
    "    # Initialize lists to hold the segmented data\n",
    "    x, y = [], []\n",
    "\n",
    "    # Loop over the dataset and create x and y using the sliding window approach\n",
    "    for i in range(0, len(data) - lookback_period - forecast_period + 1, stride):\n",
    "        x.append(data[i:i + lookback_period])\n",
    "        y.append(data[i + lookback_period:i + lookback_period + forecast_period])\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    #creating new axis so data is (#samples, timesteps, features)\n",
    "    if len(np.shape(x)) == 2:\n",
    "        x = x[:,:,np.newaxis]\n",
    "    if len(np.shape(y)) == 2:\n",
    "        y = y[:,:,np.newaxis]\n",
    "\n",
    "    # Determine the split point for the training and testing sets\n",
    "    split_idx = int(np.shape(x)[0] * (1 - test_split))\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    x_train, x_test = x[:split_idx], x[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "xTr,yTr,xTe,yTe = train_test_split(data = univariate_data, lookback_period = lookback, forecast_period = forecast, stride = stride)\n",
    "print(xTr.shape) # (samples, length of sample = lookback, features for timestep)\n",
    "print(xTe.shape)\n",
    "print(yTr.shape) # (samples, length of forecast = forecast, 1) Predicting 1 feature\n",
    "print(yTe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting the univariate data train and test samples to verify**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sequential_samples(univariate_data, x_train, y_train, x_test, y_test, model_preds_dict=None, delta_t=0.1):\n",
    "    \"\"\"\n",
    "    Plots the original univariate data along with sequentially plotted x_train, y_train, x_test, y_test,\n",
    "    and model predictions from multiple models.\n",
    "\n",
    "    Parameters:\n",
    "    - univariate_data: Original univariate time series data (1D array).\n",
    "    - x_train: Training input data (2D array).\n",
    "    - y_train: Training output data (2D array).\n",
    "    - x_test: Testing input data (2D array).\n",
    "    - y_test: Testing output data (2D array).\n",
    "    - model_preds_dict: A dictionary with keys 'training' and 'testing', each containing a list of tuples\n",
    "                        (model name, predictions) for training and testing.\n",
    "    - delta_t: The time interval between consecutive time steps.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # Define consistent colors for each model\n",
    "    model_colors = {\n",
    "        \"Weighted Average Model\": \"orange\",\n",
    "        \"LSTM Model\": \"purple\",\n",
    "        # Add more models and colors here if needed\n",
    "    }\n",
    "\n",
    "    # Plot original univariate data timeline\n",
    "    t_original = np.arange(len(univariate_data)) * delta_t\n",
    "    \n",
    "    # Plot training data (x_train and y_train) sequentially\n",
    "    lookback_period = x_train.shape[1]\n",
    "    forecast_period = y_train.shape[1]\n",
    "    start = 0\n",
    "    \n",
    "    for i in range(x_train.shape[0]):\n",
    "        t_x_train = t_original[start:start + lookback_period]\n",
    "        start += lookback_period\n",
    "        t_y_train = t_original[start:start + forecast_period]\n",
    "        start += forecast_period\n",
    "\n",
    "        # Plot true training input and output\n",
    "        plt.plot(t_x_train, x_train[i], color='blue', alpha=0.3, linestyle='-', label='xTrain' if i == 0 else '')\n",
    "        plt.plot(t_y_train, y_train[i], color='red', marker='o', markersize=2, label='yTrain (True)' if i == 0 else '')\n",
    "\n",
    "        # Plot predictions for each model on the training data\n",
    "        if model_preds_dict is not None:\n",
    "            for model_name, train_preds in model_preds_dict.get('training', []):\n",
    "                color = model_colors.get(model_name, 'black')  # Default to black if model not in the dictionary\n",
    "                plt.plot(t_y_train, train_preds[i], color=color, linestyle='-', marker='o', markersize=4, label=f'{model_name} (Train Pred)' if i == 0 else '')\n",
    "\n",
    "    # Plot testing data (x_test and y_test) sequentially\n",
    "    for i in range(x_test.shape[0]):\n",
    "        t_x_test = t_original[start:start + lookback_period]\n",
    "        start += lookback_period\n",
    "        t_y_test = t_original[start:start + forecast_period]\n",
    "        start += forecast_period\n",
    "\n",
    "        # Plot true testing input and output\n",
    "        plt.plot(t_x_test, x_test[i], color='black', alpha=0.3, linestyle='-', label='xTest' if i == 0 else '')\n",
    "        plt.plot(t_y_test, y_test[i], color='green', marker='o', markersize=2, label='yTest (True)' if i == 0 else '')\n",
    "\n",
    "        # Plot predictions for each model on the testing data\n",
    "        if model_preds_dict is not None:\n",
    "            for model_name, test_preds in model_preds_dict.get('testing', []):\n",
    "                color = model_colors.get(model_name, 'black')  # Default to black if model not in the dictionary\n",
    "                plt.plot(t_y_test, test_preds[i], color=color, linestyle='-', marker='o', markersize=4, label=f'{model_name} (Test Pred)' if i == 0 else '')\n",
    "\n",
    "    # Define the split point for shading the testing data\n",
    "    train_end_idx = len(x_train) * lookback_period\n",
    "\n",
    "    # Shade the testing portion\n",
    "    plt.axvspan(t_original[train_end_idx], t_original[-1], color='lightgray', alpha=0.3, label=None)\n",
    "\n",
    "    # Plot a vertical dotted line to separate training and testing data\n",
    "    plt.axvline(x=t_original[train_end_idx], color='gray', linestyle='--')\n",
    "\n",
    "    # Add the label \"Testing\" near the vertical line\n",
    "    plt.text(t_original[train_end_idx] + 5, plt.gca().get_ylim()[1] * 0.9, 'Testing', color='gray')\n",
    "\n",
    "    plt.title(f\"Sequential Plot of True Data and Predictions from Models\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Signal Value\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_sequential_samples(univariate_data=univariate_data, x_train=xTr, y_train=yTr, x_test=xTe, y_test=yTe, delta_t=delta_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and Prediction Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, xTrain, yTrain, xVal=None, yVal=None, epochs=10, batch_size=32, lr=0.001, shuffle=False):\n",
    "    \"\"\"\n",
    "    General training function for PyTorch models.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (nn.Module): The PyTorch model to be trained.\n",
    "    - xTrain (numpy.ndarray): Training data of shape (num_samples, timesteps, features).\n",
    "    - yTrain (numpy.ndarray): Training labels of shape (num_samples, target_dim).\n",
    "    - xVal (numpy.ndarray, optional): Validation data of shape (num_samples, timesteps, features).\n",
    "    - yVal (numpy.ndarray, optional): Validation labels of shape (num_samples, target_dim).\n",
    "    - epochs (int, optional): Number of epochs to train (default is 10).\n",
    "    - batch_size (int, optional): Batch size (default is 32).\n",
    "    - lr (float, optional): Learning rate for the optimizer (default is 0.001).\n",
    "    - device (str, optional): Device to use for training ('cpu' or 'cuda'). If None, defaults to 'cpu'.\n",
    "    \n",
    "    Returns:\n",
    "    - training_loss_history (list): History of training loss per epoch.\n",
    "    - validation_loss_history (list): History of validation loss per epoch (if validation data is provided).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert numpy arrays to PyTorch tensors and move them to the device\n",
    "    xTrain = torch.from_numpy(xTrain).float()\n",
    "    yTrain = torch.from_numpy(yTrain).float()\n",
    "    \n",
    "    if xVal is not None and yVal is not None:\n",
    "        xVal = torch.from_numpy(xVal).float()\n",
    "        yVal = torch.from_numpy(yVal).float()\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()  # Can be changed based on the task (e.g., CrossEntropyLoss for classification)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # History to store loss per epoch\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        if shuffle:\n",
    "            # Shuffle the data at the beginning of each epoch\n",
    "            permutation = torch.randperm(xTrain.size(0))\n",
    "        else:\n",
    "            # If no shuffling, maintain original order\n",
    "            permutation = torch.arange(xTrain.size(0))\n",
    "\n",
    "        # Process each batch\n",
    "        for i in range(0, len(xTrain), batch_size):\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            x_batch = xTrain[indices]\n",
    "            y_batch = yTrain[indices]\n",
    "            \n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Average loss for the epoch\n",
    "        training_loss_history.append(epoch_loss / len(xTrain))\n",
    "\n",
    "        # Validation step (if provided)\n",
    "        if xVal is not None and yVal is not None:\n",
    "            model.eval()  # Set model to evaluation mode\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():  # No need to track gradients during validation\n",
    "                val_outputs = model(xVal)\n",
    "                val_loss = criterion(val_outputs, yVal).item()\n",
    "\n",
    "            validation_loss_history.append(val_loss)\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss/len(xTrain):.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        else:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss/len(xTrain):.4f}')\n",
    "\n",
    "    if xVal is not None and yVal is not None:\n",
    "        return training_loss_history, validation_loss_history\n",
    "    else:\n",
    "        return training_loss_history\n",
    "\n",
    "\n",
    "def predict(model, input_data):\n",
    "    \"\"\"\n",
    "    Makes predictions using the provided PyTorch model and input data.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (nn.Module): The PyTorch model used for predictions.\n",
    "    - input_data (numpy.ndarray): Input data of shape (batch_size, timesteps, features).\n",
    "    \n",
    "    Returns:\n",
    "    - output (numpy.ndarray): Model predictions.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    input_tensor = torch.from_numpy(input_data).float()  # Convert numpy to PyTorch tensor\n",
    "\n",
    "    with torch.no_grad():  # No need to calculate gradients for predictions\n",
    "        output_tensor = model(input_tensor)\n",
    "\n",
    "    return output_tensor.numpy()  # Convert back to numpy array\n",
    "\n",
    "def recursive_predict(model, input_data, nRecursive_predictions):\n",
    "    \"\"\"\n",
    "    Perform recursive prediction with a PyTorch model by using the last predicted value as the next input.\n",
    "\n",
    "    Parameters:\n",
    "    - model (nn.Module): The PyTorch model used for predictions.\n",
    "    - input_data (numpy.ndarray): Initial input data of shape (batch_size, timesteps, features).\n",
    "    - nRecursive_predictions (int): Number of recursive predictions to make.\n",
    "    \n",
    "    Returns:\n",
    "    - predictions (numpy.ndarray): Array of recursive predictions.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    current_input = torch.from_numpy(input_data).float()  # Convert to tensor\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    for _ in range(nRecursive_predictions):\n",
    "        with torch.no_grad():\n",
    "            # Predict the next timestep\n",
    "            next_pred = model(current_input)\n",
    "            predictions.append(next_pred.numpy())  # Store prediction\n",
    "\n",
    "        # Shift the input sequence up by 1\n",
    "        current_input = current_input.roll(shifts=-1, dims=1)\n",
    "        # Replace the last timestep with the new prediction\n",
    "        current_input[:, -1, :] = next_pred\n",
    "\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This model requires no training\n",
    "Predicts the next point as an average of the training points\n",
    "Works with multivarate data\n",
    "Creates an mean vector for each sample\n",
    "'''\n",
    "\n",
    "def average_model_predict(series):\n",
    "    return np.mean(series, axis = 1)\n",
    "\n",
    "# multivariate_test_data = xTr #np.repeat(xTr,repeats=3,axis = -1)\n",
    "# multivariate_test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighted Average Model:**\n",
    "*Weights are learned during back propagation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WeightedAverageModel(nn.Module):\n",
    "    def __init__(self, timesteps, features):\n",
    "        \"\"\"\n",
    "        Weighted Average Model where the weights across timesteps are learned.\n",
    "\n",
    "        Parameters:\n",
    "        - timesteps (int): Number of timesteps in the input sequence.\n",
    "        - features (int): Number of features in each timestep.\n",
    "        \"\"\"\n",
    "        super(WeightedAverageModel, self).__init__()\n",
    "        \n",
    "        # Trainable weights for the timesteps\n",
    "        self.weights = nn.Parameter(torch.randn(timesteps))  # Initialized randomly\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the weighted average model.\n",
    "        \n",
    "        Parameters:\n",
    "        - x (torch.Tensor): Input tensor of shape (num_samples, timesteps, features).\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: Weighted average tensor of shape (num_samples, 1, features).\n",
    "        \"\"\"\n",
    "        # Apply softmax to ensure the weights sum to 1 and are positive\n",
    "        softmax_weights = F.softmax(self.weights, dim=0)  # Shape (timesteps,)\n",
    "        \n",
    "        # Reshape weights to (1, timesteps, 1) to broadcast them across the batch and features\n",
    "        weighted_input = x * softmax_weights.view(1, -1, 1)\n",
    "        \n",
    "        # Sum across the timesteps to get a weighted average\n",
    "        weighted_average = torch.sum(weighted_input, dim=1, keepdim=True)  # Shape (num_samples, 1, features)\n",
    "        \n",
    "        return weighted_average\n",
    "    \n",
    "# # Example usage\n",
    "# multivariate_test_data = xTr #np.repeat(xTr,repeats=3,axis = -1)\n",
    "# multivariate_test_data.shape\n",
    "# input_data = torch.from_numpy(multivariate_test_data).float()\n",
    "# print(f\"Input shape: {input_data.shape}\")\n",
    "# weighted_average_model = WeightedAverageModel(timesteps=input_data.shape[1], features=input_data.shape[2])\n",
    "# output = weighted_average_model(input_data)\n",
    "# print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM Model**\n",
    "*Single Value Prediction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMUnivariateMultiPred(nn.Module):\n",
    "    def __init__(self, timesteps, features, lstm_units=64, predict_steps=1):\n",
    "        \"\"\"\n",
    "        LSTM Model for univariate time series with multi-step prediction.\n",
    "\n",
    "        Parameters:\n",
    "        - timesteps (int): Number of timesteps in the input sequence.\n",
    "        - features (int): Number of features at each timestep.\n",
    "        - lstm_units (int): Number of units in the LSTM layer.\n",
    "        - predict_steps (int): Number of steps to predict into the future.\n",
    "        \"\"\"\n",
    "        super(LSTMUnivariateMultiPred, self).__init__()\n",
    "        \n",
    "        self.timesteps = timesteps\n",
    "        self.features = features\n",
    "        self.lstm_units = lstm_units\n",
    "        self.predict_steps = predict_steps  # Number of timesteps to predict\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=features, hidden_size=lstm_units, batch_first=True)\n",
    "\n",
    "        # Define the fully connected layer to map LSTM outputs to the correct number of features\n",
    "        # This will map from LSTM units to the desired number of features for each timestep\n",
    "        self.fc = nn.Linear(lstm_units, features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Parameters:\n",
    "        - x (torch.Tensor): Input tensor of shape (batch_size, timesteps, features).\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: Predicted output for the next `predict_steps` timesteps,\n",
    "                        shape (batch_size, predict_steps, features).\n",
    "        \"\"\"\n",
    "        # LSTM expects input of shape (batch_size, timesteps, features)\n",
    "        lstm_out, _ = self.lstm(x)  # lstm_out shape: (batch_size, timesteps, lstm_units)\n",
    "\n",
    "        # We pass the output through the fully connected layer to get the features at each timestep\n",
    "        # The fully connected layer is applied across all timesteps\n",
    "        output = self.fc(lstm_out)  # shape: (batch_size, timesteps, features)\n",
    "\n",
    "        # For multi-step prediction, output the last `predict_steps` timesteps\n",
    "        if self.predict_steps > 1:\n",
    "            output = output[:, -self.predict_steps:, :]  # shape: (batch_size, predict_steps, features)\n",
    "        else:\n",
    "            output = output[:, -1:, :]  # Ensuring the shape is (batch_size, 1, features) for single step\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training weighted average + LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data is in xTr, yTr, xTe, yTe\n",
    "'''\n",
    "timesteps = xTr.shape[1]\n",
    "features = xTr.shape[2]\n",
    "\n",
    "weighted_average_model = WeightedAverageModel(timesteps=timesteps, features=features)\n",
    "lstm_model = LSTMUnivariateMultiPred(timesteps=timesteps, features=features, lstm_units=64, predict_steps=forecast)\n",
    "\n",
    "# Train the Weighted Average Model and capture the loss history\n",
    "print(\"Training the Weighted Average Model...\")\n",
    "weighted_average_train_loss, weighted_average_val_loss = train(\n",
    "    model=weighted_average_model,\n",
    "    xTrain=xTr,\n",
    "    yTrain=yTr,  \n",
    "    xVal=xTe,  # Validation data\n",
    "    yVal=yTe,  # Validation labels\n",
    "    epochs=10,          \n",
    "    batch_size=1,      \n",
    "    lr=0.001,           \n",
    ")\n",
    "\n",
    "# Train the LSTM Model and capture the loss history\n",
    "print(\"Training the LSTM Model...\")\n",
    "lstm_train_loss, lstm_val_loss = train(\n",
    "    model=lstm_model,\n",
    "    xTrain=xTr,\n",
    "    yTrain=yTr,  \n",
    "    xVal=xTe,  # Validation data\n",
    "    yVal=yTe,  # Validation labels\n",
    "    epochs=10,          \n",
    "    batch_size=1,      \n",
    "    lr=0.001,           \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and Validation Loss Curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss curves for the Weighted Average Model\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Weighted Average Model loss curves\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(weighted_average_train_loss, label='Training Loss')\n",
    "plt.plot(weighted_average_val_loss, label='Validation Loss')\n",
    "plt.title(\"Weighted Average Model Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# LSTM Model loss curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lstm_train_loss, label='Training Loss')\n",
    "plt.plot(lstm_val_loss, label='Validation Loss')\n",
    "plt.title(\"LSTM Model Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on training and test sets for both models\n",
    "model_train_preds_weighted = predict(weighted_average_model, xTr)\n",
    "model_test_preds_weighted = predict(weighted_average_model, xTe)\n",
    "\n",
    "model_train_preds_lstm = predict(lstm_model, xTr)\n",
    "model_test_preds_lstm = predict(lstm_model, xTe)\n",
    "\n",
    "# Create a dictionary with both training and test predictions for each model\n",
    "model_predictions = {\n",
    "    \"training\": [\n",
    "        (\"Weighted Average Model\", model_train_preds_weighted),\n",
    "        (\"LSTM Model\", model_train_preds_lstm)\n",
    "    ],\n",
    "    \"testing\": [\n",
    "        (\"Weighted Average Model\", model_test_preds_weighted),\n",
    "        (\"LSTM Model\", model_test_preds_lstm)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Plot predictions from both models\n",
    "print(\"Plotting results from both models...\")\n",
    "plot_sequential_samples(univariate_data=univariate_data, x_train=xTr, y_train=yTr, x_test=xTe, y_test=yTe, model_preds_dict=model_predictions, delta_t=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
